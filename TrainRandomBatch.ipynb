{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-14T08:05:35.737319Z",
     "start_time": "2025-09-14T08:05:35.721462Z"
    }
   },
   "source": [
    "from bpe import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file='./output/tokenizer/tokenzier_v1.model')\n",
    "\n",
    "def get_vocab_size(tokenizer_param: BasicTokenizer):\n",
    "    return len(tokenizer_param.vocab) + len(tokenizer_param.special_tokens)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:05:49.568750Z",
     "start_time": "2025-09-14T08:05:47.853801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.manual_seed(3462)\n",
    "\n",
    "print(f\"PyTorch версия: {torch.__version__}\")\n",
    "print(f\"CUDA версия: {torch.version.cuda}\")\n",
    "print(f\"CuDNN версия: {torch.backends.cudnn.version()}\")"
   ],
   "id": "6a66ff7510a5344",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch версия: 2.8.0+cu126\n",
      "CUDA версия: 12.6\n",
      "CuDNN версия: 91002\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:06:28.231371Z",
     "start_time": "2025-09-14T08:06:25.529493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPTLanguageModel\n",
    "\n",
    "block_size = 256\n",
    "n_embedding = 512\n",
    "n_head = 8\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "batch_size = 64\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = GPTLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=block_size,\n",
    "    n_embeddings=n_embedding,\n",
    "    n_head=n_head,\n",
    "    device=device,\n",
    "    n_layers=n_layer,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.to(device)\n",
    "model = torch.compile(model)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "device"
   ],
   "id": "e4531399b2cf816e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.79329 M parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T17:04:16.274430Z",
     "start_time": "2025-09-14T17:04:15.801640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint_path = f'./output/pretrain/v3/checkpoint900.pth'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ],
   "id": "8c9e8c07a7849820",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 403
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:08:21.032697Z",
     "start_time": "2025-09-14T08:06:49.436856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('./output/text_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    text_corpus = f.read()\n",
    "\n",
    "encoded_text = tokenizer.encode(text_corpus)\n",
    "len(encoded_text)"
   ],
   "id": "426876845257ea77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255729"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:08:22.660334Z",
     "start_time": "2025-09-14T08:08:22.638173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.tensor(encoded_text, dtype=torch.long)\n",
    "split_index = int(0.9*len(data))\n",
    "train_data = data[:split_index]\n",
    "valid_data = data[split_index:]"
   ],
   "id": "62dfbdf2d5fbfb11",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:26:06.080910Z",
     "start_time": "2025-09-14T08:26:06.076090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def get_batch(split: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "    date = train_data if split == 'train' else valid_data\n",
    "    index = torch.randint(len(data) - block_size, (batch_size,) )\n",
    "    x = torch.stack([data[i:i+block_size] for i in index])\n",
    "    y = torch.stack([data[i+1:i+1+block_size] for i in index])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x,y"
   ],
   "id": "76728d843553287a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:26:07.569204Z",
     "start_time": "2025-09-14T08:26:07.557579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x,y = get_batch('train')\n",
    "x.shape, y.shape"
   ],
   "id": "60bd067b5fab8d15",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256]), torch.Size([64, 256]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:46:35.228043Z",
     "start_time": "2025-09-14T08:46:35.222491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Dict\n",
    "\n",
    "eval_iters = 200\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss() -> Dict:\n",
    "    output = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'valid']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x,y = get_batch(split)\n",
    "            _, loss = model(x,y)\n",
    "            losses[k] = loss\n",
    "        output[split] = losses\n",
    "    model.train()\n",
    "    return output"
   ],
   "id": "5edd448d0a076275",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:45:25.928868Z",
     "start_time": "2025-09-14T08:45:25.917019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_checkpoint(model: torch.nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    epoch: int,\n",
    "                    loss: float,\n",
    "                    file_path: str = 'checkpoint.pth'\n",
    "                    ) -> None:\n",
    "    checkpoint = {'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': loss\n",
    "                  }\n",
    "    torch.save(checkpoint, file_path)"
   ],
   "id": "9a02705b11ae552d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:48:49.051685Z",
     "start_time": "2025-09-14T08:48:39.099520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "max_iters = 1000\n",
    "eval_intervals = 10\n",
    "learning_rate = 1e-4\n",
    "save_intervals = 100\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for iteration in tqdm(range(max_iters)):\n",
    "    if iteration % eval_intervals == 0 or iteration == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f'step {iteration}'\n",
    "              f'train loss: {losses['train']:.4f}'\n",
    "              f'valid loss: {losses[\"valid\"]:.4f}')\n",
    "        train_losses.append(losses['train'])\n",
    "        valid_losses.append(losses[\"valid\"])\n",
    "\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "    logits, loss = model(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if iteration % save_intervals == 0:\n",
    "        save_checkpoint(model, optimizer, iteration, loss, f'./output/pretrain/v3/checkpoint{iteration}.pth')\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "8b0590669ad1111c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TritonMissing",
     "evalue": "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTritonMissing\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     12\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m iteration \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(max_iters)):\n\u001B[32m     13\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m iteration % eval_intervals == \u001B[32m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m iteration == max_iters - \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m         losses = \u001B[43mestimate_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mstep \u001B[39m\u001B[38;5;132;01m{\u001B[39;00miteration\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\n\u001B[32m     16\u001B[39m               \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mtrain loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[33m'\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\n\u001B[32m     17\u001B[39m               \u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33mvalid loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses[\u001B[33m\"\u001B[39m\u001B[33mvalid\u001B[39m\u001B[33m\"\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m)\n\u001B[32m     18\u001B[39m         train_losses.append(losses[\u001B[33m'\u001B[39m\u001B[33mtrain\u001B[39m\u001B[33m'\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 13\u001B[39m, in \u001B[36mestimate_loss\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(eval_iters):\n\u001B[32m     12\u001B[39m     x,y = get_batch(split)\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m     _, loss = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     14\u001B[39m     losses[k] = loss\n\u001B[32m     15\u001B[39m output[split] = losses\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:375\u001B[39m, in \u001B[36mOptimizedModule.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    365\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.nn.modules.module._has_any_global_hook():\n\u001B[32m    366\u001B[39m     warnings.warn(\n\u001B[32m    367\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mUsing `torch.compile(module)` when there are global hooks on \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    368\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mmodules (e.g., from `register_module_forward_hook`); this will\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    373\u001B[39m         stacklevel=\u001B[32m2\u001B[39m,\n\u001B[32m    374\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m375\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:749\u001B[39m, in \u001B[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    745\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(\u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m__cause__\u001B[39;00m  \u001B[38;5;66;03m# User compiler error\u001B[39;00m\n\u001B[32m    746\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ShortenTraceback \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    747\u001B[39m     \u001B[38;5;66;03m# Failures in the backend likely don't have useful\u001B[39;00m\n\u001B[32m    748\u001B[39m     \u001B[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m749\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m e.remove_dynamo_frames() \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001B[39;00m\n\u001B[32m    750\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    751\u001B[39m     \u001B[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001B[39;00m\n\u001B[32m    752\u001B[39m     set_eval_frame(\u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PyCharmMiscProject\\.venv\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:4042\u001B[39m, in \u001B[36mScheduler.create_backend\u001B[39m\u001B[34m(self, device)\u001B[39m\n\u001B[32m   4040\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m GPUTooOldForTriton(device_props, inspect.currentframe())\n\u001B[32m   4041\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m is_gpu(device.type) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m device.type == \u001B[33m\"\u001B[39m\u001B[33mmps\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m4042\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m TritonMissing(inspect.currentframe())\n\u001B[32m   4044\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m device_scheduling(\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[31mTritonMissing\u001B[39m: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T12:53:44.235983Z",
     "start_time": "2025-09-14T12:53:43.805936Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "ad5df18f7c3a746a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T08:45:13.364033Z",
     "start_time": "2025-09-14T08:45:12.066561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
    "plt.plot(valid_losses, label=\"Validation Loss\", marker='o')\n",
    "plt.xlabel(\"Evaluation Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "a4f068acfa969039",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      3\u001B[39m plt.figure(figsize=(\u001B[32m10\u001B[39m, \u001B[32m5\u001B[39m))\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m plt.plot(\u001B[43mtrain_losses\u001B[49m, label=\u001B[33m\"\u001B[39m\u001B[33mTrain Loss\u001B[39m\u001B[33m\"\u001B[39m, marker=\u001B[33m'\u001B[39m\u001B[33mo\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      5\u001B[39m plt.plot(valid_losses, label=\u001B[33m\"\u001B[39m\u001B[33mValidation Loss\u001B[39m\u001B[33m\"\u001B[39m, marker=\u001B[33m'\u001B[39m\u001B[33mo\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      6\u001B[39m plt.xlabel(\u001B[33m\"\u001B[39m\u001B[33mEvaluation Step\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T17:04:36.766206Z",
     "start_time": "2025-09-14T17:04:36.003287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_tokens = tokenizer.encode('Что нового?')\n",
    "input_tokens = torch.tensor(input_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_tokens, 50)\n",
    "a = output[0]\n",
    "print(tokenizer.decode(a.tolist()))"
   ],
   "id": "77dc30e3933bad49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Что нового?\\nМы ростелекома)ОбъявлосьУважаемые соседи! А что нужно с мобильным мужелини из 3 секциегоа не трети и в другой день! А� гу\n"
     ]
    }
   ],
   "execution_count": 405
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c709c5f71a24bccc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
